---
slug: sane-tts
title: "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech"
description: 안정적이고 자연스러운 다국어 TTS 모델인 SANE-TTS를 소개합니다.
image: img/mindslab_default.png
authors: [hyunjae, wonbin, junjun3518, tonyswoo]
tags: [publication, paper-review]
---

import clsx from 'clsx';
import styles from '../blog.module.css';

import figvits_training from './image/vits_training.png'
import figvits_inference from './image/vits_inference.png'
import figsane_tts_training from './image/sane_tts_training.png'
import figsane_tts_inference from './image/sane_tts_inference.png'
import figdataset from './image/dataset.png'
import figablation from './image/ablation.png'
import figcompare from './image/compare.png'
import figSDP from './image/SDP.png' 
import figt_SNE from './image/t_SNE.png' 
import figcontent from './image/content.png' 

[![arXiv](https://img.shields.io/badge/arXiv-2206.12132-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2206.12132)
[![githubio](https://img.shields.io/static/v1?message=Audio%20Samples&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://mindslab-ai.github.io/sane-tts/)

> Hyunjae Cho, Wonbin Jung, Junhyeok Lee, and Sang Hoon Woo. "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech." 
>
> Interspeech 2022

## SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech

안녕하세요. MINDsLab Brain팀에서 음성합성 연구를 하고 있는 조현재입니다.

오늘은 저희 Brain 팀에서 출판하고 [INTERSPEECH 2022](https://www.interspeech2022.org)에 accept된 SANE-TTS를 소개해드리는 시간을 가지려 합니다.

### Contributions

- SANE-TTS는 안정적이고 자연스러운 다국어 음성을 합성합니다.
- 이 논문에서 제안한 *speaker regularization loss*는 기존 다국어 TTS 모델에서도 사용된 방법인 domain adversarial training과 비슷한 수준의 퀄리티 향상을 이끌어냅니다.

<!--truncate-->

### Introductions

다국어 (Multilingual) TTS 모델을 훈련시키는 가장 간단한 방법은 훈련 데이터셋 내에 있는 모든 화자가 모든 언어를 다룰줄 아는 다국어 화자 데이터셋을 사용하는 것입니다.
하지만 이러한 다국어 데이터셋은 현실적으로 구하기 어렵기 때문에 대신 여러 단일언어 데이터셋을 하나로 묶은다음 훈련에 사용합니다.
예를 들어, 한국어와 영어를 말할 수 있는 다국어 TTS 모델을 훈련시키기 위해서는 한국어 데이터셋과 영어 데이터셋을 각자따로 준비해야 합니다.
하지만 다국어 TTS 특성상 모든 화자에 대해 훈련에서 사용된 모든 언어가 가능해야 하기 때문에 훈련 데이터셋에 영어음성만 존재하는 화자도 TTS 모델을 통해 한국어를 말할 수 있어야 합니다.
이렇게 훈련 데이터셋에서 특정 화자가 사용한 언어와 다른 언어로 TTS 모델이 음성합성하는 것을 cross-lingual 음성합성이라 부르고 다국어 TTS 모델을 훈련시킬때 cross-lingual 음성합성을 자연스럽고 안정적으로 만드는 것은 매우 중요합니다.

대부분의 기존 다국어 TTS 모델들은 Tacotron<a href="#r1"><sup>[1]</sup></a> 기반 모델입니다.
하지만 Tacotron 기반 다국어 TTS 모델들은 자기회귀적 (autoregressive)이게 alignment를 계산하는 과정에서 문제가 생길 수 있습니다.<a href="#r2"><sup>[2]</sup></a>
첫번째로 cross-lingual 음성합성에서의 attention 오류는 alignment를 잘못 계산하는 것으로 이어지게 되고 결국 단어 반복, 건너뛰기의 문제가 발생합니다.
두번째로 자기회귀적이게 attention을 생성하다보면 duration의 직접적인 제어가 어려워져 부자연스러운 duration을 생성할 가능성이 있습니다.
이러한 문제들을 피하기 위해 non-autoregressive한 모델들 중 VITS<a href="#r3"><sup>[3]</sup></a>를 우리 다국어 TTS 모델의 backbone 모델로 선택했습니다.

### VITS

|                       VITS at training                       |                      VITS at inference                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img className={styles.figCenter} src={figvits_training} alt="vits_training" /> | <img className={styles.figCenter} src={figvits_inference} alt="vits_inference" /> |

전체적인 VITS의 구조는 linear spectrogram을 posterior encoder를 이용해 latent $z$로 변환한 다음 다시 decoder를 통해 음성을 생성하는 VAE 구조입니다.
이 때 text encoder를 통해 나온 결과값과 latent $z$를 flow 모듈에 넣고 나온 결과값을 이용해 input text와 target speech간의 alignment를 계산합니다.
이 과정에서 monotonic alignment search (MAS)가 사용되며 나온 duration 정보를 통해 duration predictor를 훈련시킵니다.

Inference 과정에서는 text encoder에서 나온 결과값을 duration predictor가 예측한 duration을 통해 speech 길이에 맞게 늘려줍니다.
그 다음 flow reverse 과정과 decoder를 거쳐 speech를 합성합니다.

VITS 모델은 non-autoregressive한 모델이자 하나의 모델로 text를 speech로 만드는 end-to-end 모델입니다.
VITS에 대한 더 자세한 내용은 **[VITS post](https://mindslab-ai.github.io/blog/vits)**에서 보실 수 있습니다.

### SANE-TTS

|                     SANE-TTS at training                     |
| :----------------------------------------------------------: |
| <img className={clsx(styles.figCenter, styles.medium)} src={figsane_tts_training} alt="sane_tts_training" /> |

#### Language embedding

다국어 TTS 모델의 경우에는 여러 언어를 사용하기 때문에 language embedding을 이용해 언어를 구별합니다.
특히 text representation을 직접 받는 text encoder와 duration predictor 모듈에게만 language embedding을 제공하여 각 언어를 구별할 수 있도록 도와줍니다.
기본 VITS 세팅에서의 speaker embedding과 동일하게 language embedding 역시 convolution layer에 통과시킨뒤 text representation에 더하는 식으로 language 정보를 모듈에 줍니다.


Text encoder에서는 Nekvinda and Dusek<a href="#r5"><sup>[4]</sup></a>가 제안한 방법대로 language embedding을 통해 language 정보가 들어가 있는 parameter를 여러개 생성한 다음, text encoder의 각 부분에 더합니다.
이 방법은 하나의 text encoder 모듈이 language embedding만 제공하면 다양한 언어에 맞는 text representation을 생성하도록 최적화합니다.

#### Domain adversarial training

훈련 데이터에서 화자마다 사용하는 text가 다르기 때문에 모델을 훈련하는 과정에서 text 정보와 화자 정보가 서로 영향을 줄 수 있습니다.
Text representation이 화자에 따라 편향이 생기는 것을 막기 위해 domain adversarial traing (DAT)<a href="#r6"><sup>[5]</sup></a><a href="#r7"><sup>[6]</sup></a>을 도입합니다.

DAT는 다른 다국어 TTS 모델들도 많이 사용한 방법이며 SANE-TTS에 DAT를 적용하는 자세한 방법은 다음과 같습니다.
먼저, text encoder의 결과값인 $h_{text}$과 speaker classifier를 연결합니다.
연결된 speaker classifier는 gradient reversal layer와 fully connected layer로 이루어져 있습니다.
Gradient reversarial layer는 흐르는 역전파 gradient의 부호를 반대로 바꾸는 역할을 수행합니다.
이 때 speaker classifier를 화자 분류하는 cross-entropy loss를 통해 훈련하게 된다면 오히려 $h_{text}$는 화자 정보가 없어지는 방향으로 훈련이 진행됩니다.
따라서 DAT를 적용하면 text encoder는 화자에 독립적으로 학습하게 되고 TTS 모델은 모든 text에 대해서 음성을 생성할 수 있습니다.

#### Speaker regularization loss

$$
L_{reg} = E_{k \in K}[conv(S_k)]_2
$$

다국어 TTS 모델 훈련데이터에서 보통 한 화자는 단일언어에 대해서만 음성데이터가 있기 때문에 화자 정보는 언어에 편향될 수 있습니다.
특히 duration predictor에서는 화자 정보와 언어 정보가 동시에 필요하기 때문에 화자 정보의 언어 편향이 큰 문제를 발생시킬 수 있습니다.
이에 SANE-TTS는 duration predictor 내 화자 정보의 언어 편향 문제를 해결하기 위해 *speaker regularization loss*를 추가합니다.
Speaker regularization loss의 정확한 계산식은 위와 같습니다.
$K$는 하나의 batch를 의미하며 $S_k$는 datapoint $k$에서 화자의 speaker embedding을 의미합니다.
또한 $conv$는 kernel size가 1인 convolution layer를 의미합니다.
이 loss를 통해 hidden speaker representation인 $conv(S_k)$의 평균이 언어에 관계없이 영벡터로 가게 되고 화자정보는 언어와 분리되게 됩니다.
(Speaker regularization loss의 정확한 역할 설명은 Results 부분의 Figure 4에 나와있습니다.)

#### Deterministic duration predictor

기존 VITS에서는 다양한 duration을 확률적으로 예측하는 stochastic duration predictor (SDP)를 사용했습니다.
하지만 다국어 TTS 모델의 경우 cross-lingual 음성합성의 안정성을 높이는 것이 중요하기 때문에 deterministic duration predictor (DDP)로 교체합니다.<a href="#r8"><sup>[7]</sup></a>

|                      SANE-TTS at inference                       |
| :----------------------------------------------------------: |
| <img className={clsx(styles.figCenter, styles.medium)} src={figsane_tts_inference} alt="sane_tts_inference" /> |

#### Inference procedure

Speaker regularization loss를 통해 duration predictor에 있는 hidden speaker representation들은 영벡터 가까이에 위치하게 됩니다.
이를 이용해 SANE-TTS는 cross-lingual 음성합성시 duration predictor에 speaker embedding 대신 영벡터를 집어넣습니다.
따라서 SANE-TTS는 cross-lingual 음성합성을 할 때 화자가 달라지더라도 duration predictor가 동일한 duration을 예측합니다.
이 방법을 통해 cross-lingual 음성합성의 안정성을 높이고, phoneme duration에 언어가 다른 화자 정보를 반영할때 생기는 모호함을 없앨 수 있습니다.

### Experiments

<img className={clsx(styles.figCenter, styles.medium)} src={figdataset} alt="Table1" />

훈련 데이터셋에는 영어 (EN), 한국어 (KO), 일본어 (JA), 중국어 (ZH) 화자가 있습니다.
위 표에 나와있듯이 총 472명의 화자와 203.4 시간의 음성을 사용하고 음성 데이터의 5%는 validation에 사용됩니다.
22050Hz sampling rate를 사용하고 모든 text는 g2p 과정을 거쳐 phoneme으로 변환합니다.

음성의 품질을 측정하기 위해 naturalness MOS와 similarity MOS, 이 두개의 metric을 이용합니다.
한 언어당 10명의 화자, 한 화자당 동일한 30개의 문장을 사용하여 한 모델의 품질을 측정하는데 총 1200개의 음성을 사용합니다.
MOS 측정은 [Amazon Mechanical Turk](https://www.mturk.com/)에서 진행했습니다.
MOS를 측정할때 평가자는 1~5점을 줄 수 있으며 품질이 좋을수록 더 높은 평점을 부여합니다.
Metric 하나를 측정하기 위해 1200개 평점의 평균을 구했고 95% 신뢰구간을 계산했습니다.

### Results

#### Speech synthesis quality

<img className={clsx(styles.figCenter, styles.medium)} src={figcompare} alt="Table23" />

SANE-TTS와 다른 다국어 TTS model인 Meta-learning model<a href="#r5"><sup>[4]</sup></a><a href="#r9"><sup>[8]</sup></a>를 비교한 결과 음성의 자연스러움 (naturalness MOS)과 화자의 유사성 (similarity MOS) 모두 SANE-TTS가 더 좋은 결과를 냈습니다.
특히 cross-lingual 음성합성시 SANE-TTS는 화자에 관계없이 일정한 duration을 예측하기 때문에 안정적인 음성을 합성할 수 있습니다.

#### Ablation study

<img className={clsx(styles.figCenter, styles.medium)} src={figablation} alt="Table45" />

DAT를 적용하는 것과 speaker regularization loss를 추가하는 것 모두 음성의 자연스러움과 화자의 유사성을 향상시킵니다.
반면에 SDP를 사용하는 것과 기본인 DDP를 사용하는 것에는 통계상으로 유의미한 차이가 보이지 않습니다.
하지만 SDP는 종종 부자연스러운 duration을 생성한다는 문제점이 존재합니다.
Figure 3에 나와있는 예시를 보시면 SDP가 잘못된 duration을 생성하여 합성된 음성의 중간에 긴 침묵이 나오게 됐습니다.
이런 잘못된 경우를 방지하기 위해 SANE-TTS는 비교적 안정적으로 duration을 생성하는 DDP를 사용합니다.

<img className={clsx(styles.figCenter, styles.small)} src={figSDP} alt="SDP" />

아래에 있는 Figure 4는 duration predictor의 hidden speaker representation을 t-SNE plot으로 그린 결과입니다.
Speaker regularization loss를 사용하지 않은 (b)의 경우에는 같은 언어끼리 화자 정보가 뭉치게 되지만, speaker regularization loss를 추가한 (a)의 경우 언어에 상관없이 영벡터를 중심으로 분포하고 있습니다.
이 t-SNE plot은 다국어 TTS 모델에 speaker regularization loss를 추가하면 화자 정보가 언어와 분리되고 영벡터가 hidden speaker representation의 대표 역할을 할 수 있음을 보여주고 있습니다.

<img className={clsx(styles.figCenter, styles.medium)} src={figt_SNE} alt="t-SNE" />

### Audio samples

<img className={clsx(styles.figCenter, styles.small)} src={figcontent} alt="content" />

**[Github sample page](https://mindslab-ai.github.io/sane-tts/)**에는 SANE-TTS로 합성한 기본적인 다국어 음성을 포함하여 다른 다국어 TTS 모델과의 비교, ablation study에서 사용한 음성들이 있습니다.
그 외에도 약 500단어의 긴 문장을 음성으로 합성한 것과 논문에는 나와있지 않지만 code-switching을 구현한 것들도 들을 수 있습니다.

### References

<a name="r1"></a>

1. J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis, and Y. Wu, “Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4779–4783. [[arxiv]](https://arxiv.org/abs/1712.05884)

<a name="r2"></a>

2. Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “FastSpeech: Fast, Robust and Controllable Text to Speech,” in Advances in Neural Information Processing Systems, vol. 32, 2019. [[arxiv]](https://arxiv.org/abs/1905.09263)

<a name="r3"></a>

3. J. Kim, J. Kong, and J. Son, “Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech,” in International Conference on Machine Learning, vol. 139, 2021, pp.5530–5540. [[arxiv]](https://arxiv.org/abs/2106.06103)

<a name="r4"></a>

4. T. Nekvinda and O. Dusek, “One Model, Many Languages: Meta-Learning for Multilingual Text-to-Speech,” in INTERSPEECH, 2020, pp. 2972–2976. [[arxiv]](https://arxiv.org/abs/2008.00768)

<a name="r5"></a>

5. Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Z. Chen, R. Skerry-Ryan, Y. Jia, A. Rosenberg, and B. Ramabhadran, “Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning,” in INTERSPEECH, 2019, pp. 2080–2084. [[arxiv]](https://arxiv.org/abs/1907.04448)

<a name="r6"></a>

6. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky, “Domain-Adversarial Training of Neural Networks,” Journal of Machine Learning Research, vol. 17, no. 59, pp. 1–35, 2016. [[arxiv]](https://arxiv.org/abs/1505.07818)

<a name="r7"></a>

7. E. Casanova, J. Weber, C. Shulby, A. C. Junior, E. Golge, and M. A. Ponti, “YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone,” arXiv preprint arXiv:2112.02418, 2021. [[arxiv]](https://arxiv.org/abs/2112.02418)

<a name="r8"></a>

8. Official source code of Meta-learning model: [[github]](https://github.com/Tomiinek/Multilingual_Text_to_Speech)