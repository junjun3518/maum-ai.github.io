"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[252],{3905:function(e,t,a){a.d(t,{Zo:function(){return u},kt:function(){return h}});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,o=e.originalType,l=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),m=c(a),h=s,k=m["".concat(l,".").concat(h)]||m[h]||p[h]||o;return a?n.createElement(k,i(i({ref:t},u),{},{components:a})):n.createElement(k,i({ref:t},u))}));function h(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var o=a.length,i=new Array(o);i[0]=m;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:s,i[1]=r;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8202:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return y},default:function(){return P},frontMatter:function(){return b},metadata:function(){return v},toc:function(){return N}});var n=a(7462),s=a(3366),o=(a(7294),a(3905)),i="category_GWcC",r="publications_acVk",l="conference_QDfE",c="authors_KmdO",u="github_vEIJ",p="demo_M62R",m="misc_vWNW",h="description_MqPA",k="full_aKhn",d="expanded_RYzr",f="trigger_WWqs",g=["components"],b={title:"Publications",description:"List of Publications from MINDsLab Brain Team"},y=void 0,v={type:"mdx",permalink:"/publications",source:"@site/src/pages/publications.mdx",title:"Publications",description:"List of Publications from MINDsLab Brain Team",frontMatter:{title:"Publications",description:"List of Publications from MINDsLab Brain Team"}},N=[{value:"Publications",id:"publications",level:2},{value:"2022",id:"2022",level:3},{value:"2021",id:"2021",level:3},{value:"2020",id:"2020",level:3}],w={toc:N};function P(e){var t=e.components,a=(0,s.Z)(e,g);return(0,o.kt)("wrapper",(0,n.Z)({},w,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"publications"},"Publications"),(0,o.kt)("h3",{id:"2022"},"2022"),(0,o.kt)("section",{id:"activities",className:i},(0,o.kt)("ul",{className:r},(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"CVPR Demo"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2205.06421"},"Talking Face Generation with Multilingual TTS")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Hyoung-Kyu Song",(0,o.kt)("sup",null,"*")),", ",(0,o.kt)("b",null,"Sang Hoon Woo",(0,o.kt)("sup",null,"*")),", ",(0,o.kt)("b",null,"Junhyeok Lee"),", ",(0,o.kt)("b",null,"Seungmin Yang"),", ",(0,o.kt)("b",null,"Hyunjae Cho"),", ",(0,o.kt)("b",null,"Dongho Choi"),", ",(0,o.kt)("b",null,"Kang-wook Kim"),", and ",(0,o.kt)("b",null,"Youseong Lee")),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2022-cvpr-demo"}),(0,o.kt)("p",{className:h},"In this work, we propose a joint system combining a talking face generation system with a text-to-speech system that can generate multilingual talking face videos from only the text input. ",(0,o.kt)("span",{className:k},"Our system can synthesize natural multilingual speeches while maintaining the vocal identity of the speaker, as well as lip movements synchronized to the synthesized speech. We demonstrate the generalization capabilities of our system by selecting four languages (Korean, English, Japanese, and Chinese) each from a different language family. We also compare the outputs of our talking face generation model to outputs of a prior work that claims multilingual support. For our demo, we add a translation API to the preprocessing stage and present it in the form of a neural dubber so that users can utilize the multilingual property of our system more easily.")),(0,o.kt)("label",{for:"post-2022-cvpr-demo",className:f}))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"CVPR"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2106.07023"},"Styleformer: Transformer based Generative Adversarial Networks with Style Vector")),(0,o.kt)("p",{className:c},"Jeeseung Park",(0,o.kt)("sup",null,"*"),", and ",(0,o.kt)("b",null,"Younggeun Kim",(0,o.kt)("sup",null,"*"))),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2022-cvpr"}),(0,o.kt)("p",{className:h},"We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. ",(0,o.kt)("span",{className:k},"In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA.")),(0,o.kt)("label",{for:"post-2022-cvpr",className:f})),(0,o.kt)("p",{className:u},(0,o.kt)("a",{href:"https://github.com/Jeeseung-Park/Styleformer"},"GitHub"))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"ICASSP"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2104.00931"},"Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Kang-wook Kim",(0,o.kt)("sup",null,"*")),", ",(0,o.kt)("b",null,"Seung-won Park"),", ",(0,o.kt)("b",null,"Junhyeok Lee"),", and ",(0,o.kt)("b",null,"Myun-chul Joe")),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2022-icassp"}),(0,o.kt)("p",{className:h},"Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. ",(0,o.kt)("span",{className:k},"To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training.")),(0,o.kt)("label",{for:"post-2022-icassp",className:f})),(0,o.kt)("p",{className:u},(0,o.kt)("a",{href:"https://github.com/mindslab-ai/assem-vc"},"GitHub"))))),(0,o.kt)("h3",{id:"2021"},"2021"),(0,o.kt)("section",{id:"activities",className:i},(0,o.kt)("ul",{className:r},(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"NeurIPS Workshop (Oral)"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2110.12676"},"Controllable and Interpretable Singing Voice Decomposition via Assem-VC")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Kang-wook Kim",(0,o.kt)("sup",null,"*"))," and ",(0,o.kt)("b",null,"Junhyeok Lee")),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2021-nips-workshop"}),(0,o.kt)("p",{className:h},"We propose a singing decomposition system that encodes time-aligned linguistic content, pitch, and source speaker identity via Assem-VC.",(0,o.kt)("span",{className:k}," With decomposed speaker-independent information and the target speaker's embedding, we could synthesize the singing voice of the target speaker. In conclusion, we made a perfectly synced duet with the user's singing voice and the target singer's converted singing voice.")),(0,o.kt)("label",{for:"post-2021-nips-workshop",className:f})),(0,o.kt)("p",{className:u},(0,o.kt)("a",{href:"https://github.com/mindslab-ai/assem-vc"},"GitHub")),(0,o.kt)("p",{className:p},(0,o.kt)("a",{href:"https://mindslab-ai.github.io/assem-vc/singer/"},"Demo"))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"Interspeech"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2104.02321"},"NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Junhyeok Lee",(0,o.kt)("sup",null,"*"))," and ",(0,o.kt)("b",null,"Seungu Han")),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2021-is"}),(0,o.kt)("p",{className:h},"In this work, we introduce NU-Wave, the first neural audio upsampling model to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs, while prior works could generate only up to 16kHz. ",(0,o.kt)("span",{className:k},"NU-Wave is the first diffusion probabilistic model for audio super-resolution which is engineered based on neural vocoders. NU-Wave generates high-quality audio that achieves high performance in terms of signal-to-noise ratio (SNR), log-spectral distance (LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the baseline models despite the substantially smaller model capacity (3.0M parameters) than baselines (5.4-21%).")),(0,o.kt)("label",{for:"post-2021-is",className:f})),(0,o.kt)("p",{className:u},(0,o.kt)("a",{href:"https://github.com/mindslab-ai/nuwave"},"GitHub")),(0,o.kt)("p",{className:p},(0,o.kt)("a",{href:"https://mindslab-ai.github.io/nuwave"},"Demo"))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"CVPR Workshop"),(0,o.kt)("h3",null,(0,o.kt)("a",null,"Sharp Edge Recovery via SE(3)-Equivariant Network")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Youseong Lee",(0,o.kt)("sup",null,"*"))),(0,o.kt)("p",{className:m},(0,o.kt)("a",{href:"https://youtu.be/UVYQzQ-mH1w?t=9231"},"Talk"))))),(0,o.kt)("h3",{id:"2020"},"2020"),(0,o.kt)("section",{id:"activities",className:i},(0,o.kt)("ul",{className:r},(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"NeurIPS Workshop"),(0,o.kt)("h3",null,(0,o.kt)("a",null,"DS4C Patient Policy Province Dataset: A Comprehensive COVID-19 Dataset for Causal and Epidemiological Analysis")),(0,o.kt)("p",{className:c},"Jimi Kim",(0,o.kt)("sup",null,"*"),", ",(0,o.kt)("b",null,"DongHwan Jang"),", Seojin Jang, Woncheol Lee, and ",(0,o.kt)("b",null,"Joong Kun Lee")),(0,o.kt)("p",{className:m},(0,o.kt)("a",{href:"https://www.cmu.edu/dietrich/causality/neurips20ws/"},"Workshop"))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"Interspeech"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2005.03295"},"Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Seung-won Park",(0,o.kt)("sup",null,"*")),", ",(0,o.kt)("b",null,"Doo-young Kim"),", and ",(0,o.kt)("b",null,"Myun-chul Joe")),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2020-is"}),(0,o.kt)("p",{className:h},"We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. ",(0,o.kt)("span",{className:k},"Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance.")),(0,o.kt)("label",{for:"post-2020-is",className:f})),(0,o.kt)("p",{className:u},(0,o.kt)("a",{href:"https://github.com/mindslab-ai/cotatron"},"GitHub")),(0,o.kt)("p",{className:p},(0,o.kt)("a",{href:"https://mindslab-ai.github.io/cotatron/"},"Demo")),(0,o.kt)("p",{className:m},(0,o.kt)("a",{href:"https://youtu.be/lnNuL8hqoh4"},"Talk"))),(0,o.kt)("li",null,(0,o.kt)("div",{className:l},"ECCV Workshop"),(0,o.kt)("h3",null,(0,o.kt)("a",{href:"https://arxiv.org/abs/2009.02857"},"3D Room Layout Estimation Beyond the Manhattan World Assumption")),(0,o.kt)("p",{className:c},(0,o.kt)("b",null,"Dongho Choi",(0,o.kt)("sup",null,"*"))),(0,o.kt)("div",null,(0,o.kt)("input",{type:"checkbox",className:d,id:"post-2020-eccvw"}),(0,o.kt)("p",{className:h},"Predicting 3D room layout from single image is a challenging task with many applications. ",(0,o.kt)("span",{className:k},"In this paper, we propose a new training and post-processing method for 3D room layout estimation, built on a recent state-of-the-art 3D room layout estimation model. Experimental results show our method outperforms state-of-the-art approaches by a large margin in predicting visible room layout. Our method has obtained the 3rd place in 2020 Holistic Scene Structures for 3D Vision Workshop.")),(0,o.kt)("label",{for:"post-2020-eccvw",className:f}))))))}P.isMDXComponent=!0}}]);