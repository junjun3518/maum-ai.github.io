"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[4069],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),l=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=l(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},f=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=c(e,["components","mdxType","originalType","parentName"]),f=l(n),d=r,g=f["".concat(s,".").concat(d)]||f[d]||p[d]||i;return n?a.createElement(g,o(o({ref:t},u),{},{components:n})):a.createElement(g,o({ref:t},u))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=f;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:r,o[1]=c;for(var l=2;l<i;l++)o[l]=n[l];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}f.displayName="MDXCreateElement"},983:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return s},default:function(){return d},frontMatter:function(){return c},metadata:function(){return l},toc:function(){return p}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=(n(7510),n(8441),n(6819),n(9435),n(6418),n(8611),n(4129),n(741),n(762),n(4597),n(2377),n(8746),n(1772),n(4068),n(3842),n(1736),n(6044),n(6905),n(7348),n(3334),["components"]),c={slug:"vits",title:"VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",description:"\ubcf5\uc7a1\ub2e4\ub2e8\ud55c End-to-End TTS \ubaa8\ub378\uc778 VITS\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.",image:"img/mindslab_default.png",authors:["wonbin"],tags:["paper-review"]},s=void 0,l={permalink:"/blog/vits",source:"@site/blog/2021-10-19-paper-review-vits/index.mdx",title:"VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",description:"\ubcf5\uc7a1\ub2e4\ub2e8\ud55c End-to-End TTS \ubaa8\ub378\uc778 VITS\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.",date:"2021-10-19T00:00:00.000Z",formattedDate:"October 19, 2021",tags:[{label:"paper-review",permalink:"/blog/tags/paper-review"}],readingTime:31.515,truncated:!0,authors:[{name:"Wonbin Jung",title:"AI Scientist (Audio)",url:"https://github.com/Wonbin-Jung",imageURL:"https://github.com/Wonbin-Jung.png",key:"wonbin"}],frontMatter:{slug:"vits",title:"VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",description:"\ubcf5\uc7a1\ub2e4\ub2e8\ud55c End-to-End TTS \ubaa8\ub378\uc778 VITS\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.",image:"img/mindslab_default.png",authors:["wonbin"],tags:["paper-review"]},prevItem:{title:"CES 2022\uc5d0 \ucc38\uc11d\ud55c \ub9c8\uc778\uc988\ub7a9, AI Human",permalink:"/blog/ces-2022-review"},nextItem:{title:"Activate or Not: Learning Customized Activation",permalink:"/blog/acon"}},u={authorsImageUrls:[void 0]},p=[{value:"Variational Inference with adversarial learning for end-to-end Text-to-Speech",id:"variational-inference-with-adversarial-learning-for-end-to-end-text-to-speech",level:2},{value:"Contributions",id:"contributions",level:3}],f={toc:p};function d(e){var t=e.components,n=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2106.06103"},(0,i.kt)("img",{parentName:"a",src:"https://img.shields.io/badge/arXiv-2106.06103-brightgreen.svg?style=flat-square",alt:"arXiv"})),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/jaywalnut310/vits"},(0,i.kt)("img",{parentName:"a",src:"https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square",alt:"githubio"})),"\n",(0,i.kt)("a",{parentName:"p",href:"https://jaywalnut310.github.io/vits-demo/index.html"},(0,i.kt)("img",{parentName:"a",src:"https://img.shields.io/static/v1?message=Audio%20Samples&logo=Github&labelColor=grey&color=lightgrey&logoColor=white&label=%20&style=flat-square",alt:"githubio"}))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'Kim, Jaehyeon, Jungil Kong, and Juhee Son. "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech." '),(0,i.kt)("p",{parentName:"blockquote"},"International Conference on Machine Learning (ICML) 2021")),(0,i.kt)("h2",{id:"variational-inference-with-adversarial-learning-for-end-to-end-text-to-speech"},"Variational Inference with adversarial learning for end-to-end Text-to-Speech"),(0,i.kt)("p",null,"\uc548\ub155\ud558\uc138\uc694! MINDs Lab Brain\uc5d0\uc11c text-to-speech (TTS) \uc5f0\uad6c\ub97c \ud558\uace0 \uc788\ub294 \uc815\uc6d0\ube48\uc785\ub2c8\ub2e4. \uc624\ub298\uc740 \uc9c0\ub09c \uc5ec\ub984\uc5d0 \ubc1c\ud45c\ub41c end-to-end TTS\uc778 Variational Inference with adversarial learning for end-to-end Text-to-Speech, ",(0,i.kt)("strong",{parentName:"p"},"VITS"),"\uc5d0 \ub300\ud574 \uc18c\uac1c\ud558\uace0, \ub9ac\ubdf0\ub97c \uc9c4\ud589\ud558\uace0\uc790 \ud569\ub2c8\ub2e4."),(0,i.kt)("h3",{id:"contributions"},"Contributions"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"1\ub2e8\uacc4 \ud569\uc131 \ubc0f \ubcd1\ub82c \ud2b8\ub808\uc774\ub2dd\uc774 \uac00\ub2a5\ud558\uba70 \uc131\ub2a5\uc774 \uae30\uc874 \ubaa8\ub378\uc5d0 \uacac\uc904 \uc218 \uc788\ub294 end-to-end TTS\ub97c \uc81c\uc548\ud588\uc2b5\ub2c8\ub2e4."),(0,i.kt)("li",{parentName:"ul"},"Variational Auto-Encoder (VAE)\uc758 \uad6c\uc870\ub97c \uc801\uc6a9\ud558\uc5ec 2\ub2e8\uacc4 \ud569\uc131\uc744 \ud558\ub098\ub85c \uc5f0\uacb0\uc2dc\ucf30\uc2b5\ub2c8\ub2e4."),(0,i.kt)("li",{parentName:"ul"},"Variational inference\uc5d0 normalizing flow\uc640 generative adversarial network (GAN)\uc758 adversarial training\uc744 \uacb0\ud569\uc2dc\ucf1c \ud45c\ud604\ub825\uc744 \ub192\uc600\uc2b5\ub2c8\ub2e4."),(0,i.kt)("li",{parentName:"ul"},"Stochastic duration predictor (SDP)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub79c\ub364\ud558\uac8c \uc74c\uc131\uc758 \uae38\uc774\ub97c \uc608\uce21\ud558\ubbc0\ub85c \uc74c\uc131\uc758 \ub2e4\uc591\uc131\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.")))}d.isMDXComponent=!0},7510:function(e,t){t.Z={small:"small_g5vo",medium:"medium_PfCl",figCenter:"figCenter_mKJ5"}},8441:function(e,t,n){t.Z=n.p+"assets/images/alignment-9fce05378464deb4515c8d6a94603a8d.png"},4597:function(e,t,n){t.Z=n.p+"assets/images/decoder-1a8889304e13f4eee89c7e612a0758c4.png"},1772:function(e,t,n){t.Z=n.p+"assets/images/duration_predictor-c9d1b813d78cc3a8c09f008c75a0523b.png"},6044:function(e,t,n){t.Z=n.p+"assets/images/figure2-bac8da476c2410b67626cd318c851ec2.png"},6905:function(e,t,n){t.Z=n.p+"assets/images/figure3-4c13d8273a5bd050ef71bfc1f5ff441d.png"},2377:function(e,t,n){t.Z=n.p+"assets/images/figure5-a0c4bdbbaab1943dadb8a80f3e799159.png"},8746:function(e,t,n){t.Z=n.p+"assets/images/figure6-760d53f9e0a29ed1722092819c587732.png"},3334:function(e,t,n){t.Z=n.p+"assets/images/figure7-6823fbd6a4815340cfcc3569785bb3e8.png"},9435:function(e,t,n){t.Z=n.p+"assets/images/flow-9e096fb36ed22ffdc46c1b5ebaf08338.png"},8611:function(e,t,n){n.p},6819:function(e,t,n){t.Z=n.p+"assets/images/mas-9822018e72dd37d4665a91c248fa6d80.png"},741:function(e,t,n){t.Z=n.p+"assets/images/posterior_encoder-b5c107c5c8b06e55aa15bc4f095d29e9.png"},762:function(e,t,n){t.Z=n.p+"assets/images/prior_encoder-ac253d2cac22c37f3f9f455f60eb2ac1.png"},4068:function(e,t,n){t.Z=n.p+"assets/images/table1-cede84c1c0df475113fb05b6a8f09843.png"},3842:function(e,t,n){t.Z=n.p+"assets/images/table2-5ee83d171352947313d7f5ff09b86b0a.png"},1736:function(e,t,n){t.Z=n.p+"assets/images/table3-9cbd2b231ff7a9c1dee9f23de4087bd9.png"},7348:function(e,t,n){t.Z=n.p+"assets/images/table4-29a2566d071012b2b6f7773f88001b62.png"},4129:function(e,t,n){t.Z=n.p+"assets/images/text_encoder-4b3eef4ede1cbae2861b371cc008d241.png"},6418:function(e,t,n){n.p}}]);